{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-EuotoZWWCW",
        "outputId": "eb92a62a-af0a-4ce2-a881-25b3e55d09e3"
      },
      "outputs": [],
      "source": [
        "!pip install pyarrow datasets huggingface_hub\n",
        "!pip install nlpaug\n",
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyc6IhSLc5qM"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zaby-3lWiD6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from spellchecker import SpellChecker\n",
        "from textblob import TextBlob\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "from datasets import load_dataset, load_from_disk, DownloadMode, DatasetDict\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, BatchNormalization, Bidirectional, GRU, Dense, Dropout\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.regularizers import l2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oGEqmnTc-vf"
      },
      "source": [
        "# Mounting Google Drive (Only for Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_LL4c6iWqai",
        "outputId": "84b73ec2-cbf4-47b5-9905-93f7ebf9e97e"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# Define the destination folder\n",
        "destination_path = \"/content/drive/MyDrive/Anvendt Maskinl√¶ring - Eksamen/Data/Data_problem_2\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIlLP0r5dDDx"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmZ_0XcYW3HB"
      },
      "outputs": [],
      "source": [
        "dataset_emotion = load_dataset(\n",
        "    \"dair-ai/emotion\", \"unsplit\",\n",
        "    cache_dir=destination_path,\n",
        "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN42HdazdEcB"
      },
      "source": [
        "# Initial Split of the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr-AJxPaW7QI",
        "outputId": "c67ea222-765d-4436-9f61-4b8e6da38bd2"
      },
      "outputs": [],
      "source": [
        "# Split the emotion dataset into training, and test sets\n",
        "hf_train = dataset_emotion['train'].train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
        "hf_test  = hf_train['test']\n",
        "hf_train = hf_train['train']\n",
        "\n",
        "# Split the training set into training and validation sets\n",
        "hf_train = hf_train.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "# Rename just to make it easier to understand\n",
        "hf_train = DatasetDict({\n",
        "    'validation': hf_train['test'],\n",
        "    'train': hf_train['train']\n",
        "})\n",
        "hf_validation = hf_train['validation']\n",
        "hf_train = hf_train['train']\n",
        "\n",
        "# Display the sizes of the splits\n",
        "print(f\"Train dataset size: {len(hf_train)}\")\n",
        "print(f\"Validation dataset size: {len(hf_validation)}\")\n",
        "print(f\"Test dataset size: {len(hf_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaeRo57idHow"
      },
      "source": [
        "# Counting Number of Classes in Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyUiF3-GW-_L",
        "outputId": "8e911cfc-5da4-4bb5-a6c6-f2ed274ea516"
      },
      "outputs": [],
      "source": [
        "# Count label distributions for train, validation, and test datasets\n",
        "train_labels = [example['label'] for example in hf_train]\n",
        "val_labels = [example['label'] for example in hf_validation]\n",
        "test_labels = [example['label'] for example in hf_test]\n",
        "\n",
        "# Get label distributions\n",
        "train_label_counts = Counter(train_labels)\n",
        "val_label_counts = Counter(val_labels)\n",
        "test_label_counts = Counter(test_labels)\n",
        "\n",
        "# Create a DataFrame for easier visualization\n",
        "label_distribution = pd.DataFrame({\n",
        "    \"Train\": train_label_counts,\n",
        "    \"Validation\": val_label_counts,\n",
        "    \"Test\": test_label_counts\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "# Display the distribution\n",
        "print(label_distribution)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04i2JVUhdQ63"
      },
      "source": [
        "# Visualizing the Distributions of Classes in Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "VbKjau4HXCdY",
        "outputId": "3c8b39b6-a411-424e-cdc1-7d0f95b86905"
      },
      "outputs": [],
      "source": [
        "# Define the mapping of label numbers to emotion names\n",
        "label_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Update the index of the label_distribution DataFrame with emotion names\n",
        "label_distribution_named = label_distribution.rename(index=label_mapping)\n",
        "\n",
        "# Plot separate bar plots for train, validation, and test in the same figure\n",
        "label_distribution_named.plot(\n",
        "    kind=\"bar\",\n",
        "    figsize=(12, 12),\n",
        "    subplots=True,\n",
        "    layout=(3, 1),\n",
        "    sharex=False,\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "# Add titles for each subplot\n",
        "plt.suptitle(\"Distribution of Emotions Across Train, Validation, and Test Sets\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"Emotions\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK1NmXSGdW5C"
      },
      "source": [
        "# Visualizing Tweet Lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "pS4cdMh9XI1s",
        "outputId": "3b1e03bf-c098-4ff9-fb7d-55338aea12a3"
      },
      "outputs": [],
      "source": [
        "# Measure tweet lengths\n",
        "tweet_lengths_train = [len(text.split()) for text in hf_train['text']]  # Word count per tweet\n",
        "\n",
        "# Analyze distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tweet_lengths_train, bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Distribution of Tweet Lengths in Train Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "I20FssYsXLfP",
        "outputId": "9f06d75f-c808-4f54-85ca-4719adb158bc"
      },
      "outputs": [],
      "source": [
        "# Measure tweet lengths\n",
        "tweet_lengths_validation = [len(text.split()) for text in hf_validation['text']]  # Word count per tweet\n",
        "\n",
        "# Analyze distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tweet_lengths_validation, bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Distribution of Tweet Lengths in Validation Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "wDrnXmSeXORY",
        "outputId": "6f22ac0c-52d1-4bc8-96fa-cb7568e4b38d"
      },
      "outputs": [],
      "source": [
        "# Measure tweet lengths\n",
        "tweet_lengths_test = [len(text.split()) for text in hf_test['text']]  # Word count per tweet\n",
        "\n",
        "# Analyze distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tweet_lengths_test, bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Distribution of Tweet Lengths in Test Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97hUOXW5deoH"
      },
      "source": [
        "# Concatenating the Train, Validation and Test Set into one Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-REPgakdYxRO",
        "outputId": "ad26832f-10a3-4409-8307-4bbd77687f8d"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Combine the datasets using concatenate_datasets\n",
        "combined_dataset = concatenate_datasets([hf_train, hf_validation, hf_test])\n",
        "\n",
        "print(len(combined_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZroXV85dolk"
      },
      "source": [
        "# Resplitting the Dataset, ensuring equal Distribution of Tweet Lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91rfcX9XYznM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Compute tweet lengths for all texts in combined_dataset\n",
        "tweet_lengths = [len(text.split()) for text in combined_dataset['text']]\n",
        "\n",
        "# Bin tweet lengths into categories (e.g., short, medium, long)\n",
        "tweet_length_bins = np.digitize(tweet_lengths, bins=[10, 20, 50])  # Define bin edges\n",
        "# Bins: 0 = <=10 words, 1 = 11-20 words, 2 = 21-50 words, 3 = >50 words\n",
        "\n",
        "# Perform stratified split using the bins\n",
        "train_indices, temp_indices = train_test_split(\n",
        "    range(len(combined_dataset)),\n",
        "    test_size=0.25,\n",
        "    stratify=tweet_length_bins,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Further split temp_indices into validation and test sets\n",
        "val_bins = [tweet_length_bins[i] for i in temp_indices]\n",
        "val_indices, test_indices = train_test_split(\n",
        "    temp_indices,\n",
        "    test_size=0.30,\n",
        "    stratify=val_bins,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create new DatasetDict\n",
        "recombined_dataset = DatasetDict({\n",
        "    'train': combined_dataset.select(train_indices),\n",
        "    'validation': combined_dataset.select(val_indices),\n",
        "    'test': combined_dataset.select(test_indices)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4zAUqdYdsKO"
      },
      "source": [
        "# Checking Number of Observations in new Train, Validation and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6JVIPbOY67q",
        "outputId": "186b404c-b89c-4d57-83d0-5ed570e03159"
      },
      "outputs": [],
      "source": [
        "# Assign splits to convenient variables\n",
        "hf_train = recombined_dataset['train']\n",
        "hf_validation = recombined_dataset['validation']\n",
        "hf_test = recombined_dataset['test']\n",
        "\n",
        "# Verify sizes of the splits\n",
        "print(f\"Train dataset size: {len(hf_train)}\")\n",
        "print(f\"Validation dataset size: {len(hf_validation)}\")\n",
        "print(f\"Test dataset size: {len(hf_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNdBm0WdwSB"
      },
      "source": [
        "# Visualizing the New Tweets Lengths for the Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "GQ9eevsXZhBI",
        "outputId": "39b46c88-fd36-4457-b8f8-fd5bcc846c02"
      },
      "outputs": [],
      "source": [
        "# Step 1: Measure tweet lengths\n",
        "tweet_lengths = [len(text.split()) for text in hf_train['text']]  # Word count per tweet\n",
        "\n",
        "# Step 2: Analyze distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tweet_lengths, bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Distribution of Tweet Lengths in Train Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "VHdYFFtzZjFM",
        "outputId": "3b32ecd1-7ccf-4105-92bd-73cb77b2fa98"
      },
      "outputs": [],
      "source": [
        "# Step 1: Measure tweet lengths\n",
        "tweet_lengths = [len(text.split()) for text in hf_validation['text']]  # Word count per tweet\n",
        "\n",
        "# Step 2: Analyze distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tweet_lengths, bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Distribution of Tweet Lengths in Validation Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "b0Nmy08UZlNo",
        "outputId": "4cdf40bc-1852-4639-d892-7af3720b3903"
      },
      "outputs": [],
      "source": [
        "# Step 1: Measure tweet lengths\n",
        "tweet_lengths = [len(text.split()) for text in hf_test['text']]  # Word count per tweet\n",
        "\n",
        "# Step 2: Analyze distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tweet_lengths, bins=50, color='blue', alpha=0.7)\n",
        "plt.title(\"Distribution of Tweet Lengths in Test Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuLlLmQoaXBG"
      },
      "source": [
        "# Checking for Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A4qAGLGaY0O",
        "outputId": "5086280f-06a8-40fc-9a7c-9cec1683583a"
      },
      "outputs": [],
      "source": [
        "# Combine all datasets into one list\n",
        "all_texts = hf_train['text'] + hf_validation['text'] + hf_test['text']\n",
        "\n",
        "# Count occurrences across all datasets\n",
        "all_text_counts = Counter(all_texts)\n",
        "\n",
        "# Filter tweets that appear more than once\n",
        "duplicate_tweets = [tweet for tweet, count in all_text_counts.items() if count > 1]\n",
        "\n",
        "\n",
        "# Print the first 5 duplicated tweets\n",
        "for i, tweet in enumerate(duplicate_tweets[:5]):\n",
        "    print(f\"Duplicate Tweet {i+1}: {tweet}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyR4llkXd6kY"
      },
      "source": [
        "# Print the Number of Duplicated Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGf_vLrhagdS",
        "outputId": "9cf85997-25a0-429e-9497-04af7d6c5da1"
      },
      "outputs": [],
      "source": [
        "print(len(duplicate_tweets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N2Y78Mtd-X3"
      },
      "source": [
        "# Checking the Labels of the Duplicated Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsK0NcuveHxZ",
        "outputId": "4e4289ea-b389-4c99-9a29-7cc583c10dcc"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of each tweet\n",
        "tweet_counts_train = Counter(hf_train['text'])\n",
        "tweet_counts_validation = Counter(hf_validation['text'])\n",
        "tweet_counts_test = Counter(hf_test['text'])\n",
        "\n",
        "# Combine counts from all datasets\n",
        "tweet_counts = tweet_counts_train + tweet_counts_validation + tweet_counts_test\n",
        "\n",
        "# Filter tweets that appear more than once\n",
        "duplicates = [tweet for tweet, count in tweet_counts.items() if count > 1]\n",
        "\n",
        "print(f'Number of duplicates: {len(duplicates)}')\n",
        "\n",
        "# Step 3: Display the labels assigned to the duplicated tweets\n",
        "print(\"Labels for duplicated tweets:\")\n",
        "for i, tweet in enumerate(duplicates[:5], 1):  # Display the first 5 tweets\n",
        "    # Find all indices where this tweet occurs\n",
        "    indices = [idx for idx, text in enumerate(hf_train['text']) if text == tweet]\n",
        "    # Get the associated labels for this tweet\n",
        "    labels = [hf_train['label'][idx] for idx in indices]\n",
        "    print(f\"{i}: Tweet: '{tweet}'\")\n",
        "    print(f\"   Assigned Labels: {[label_mapping[label] for label in labels]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3kTsryEeEKf"
      },
      "source": [
        "# Resolving Duplicate Tweets by Assigning the Observations the Most Frequent Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9vjClpJbO6Y",
        "outputId": "e132d2eb-25c7-482e-cb77-1cb894b7a151"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Calculate global label frequencies\n",
        "all_labels = hf_train['label'] + hf_validation['label'] + hf_test['label']\n",
        "global_label_counts = Counter(all_labels)\n",
        "print(\"Global label frequencies (across all datasets):\")\n",
        "print({label_mapping[label]: count for label, count in global_label_counts.items()})\n",
        "\n",
        "# Define specific tweets for verification\n",
        "specific_tweets = [\n",
        "    'i grabbed something to eat before the evening class feeling strange in the food court with my very high heels surrounded by students in much more comfortable footwear',\n",
        "    'i need to think of it differently this way i wont have blood gushing out of my nose and not feel overwhelmed at times where i feel as if i am being smothered',\n",
        "    'i cannot even explain it to myself so i just laugh and smile and feel amazing',\n",
        "    'i feel agitated i become easily overwhelmed',\n",
        "    'i just feel so stressed out with life'\n",
        "]\n",
        "\n",
        "def resolve_duplicates(texts, labels, dataset_name):\n",
        "    \"\"\"\n",
        "    Resolve duplicates in a dataset by assigning the most frequent global label.\n",
        "    \"\"\"\n",
        "    # Step 2: Group tweets by their associated labels\n",
        "    tweet_labels = defaultdict(list)\n",
        "    for text, label in zip(texts, labels):\n",
        "        tweet_labels[text].append(label)\n",
        "\n",
        "    # Step 3: Resolve duplicates by assigning the most frequent global label\n",
        "    resolved_texts = []\n",
        "    resolved_labels = []\n",
        "\n",
        "    for tweet, tweet_labels_list in tweet_labels.items():\n",
        "        # Count occurrences of each label for this tweet\n",
        "        label_counts = Counter(tweet_labels_list)\n",
        "        # Assign the most globally frequent label\n",
        "        most_frequent_label = max(label_counts, key=lambda label: global_label_counts[label])\n",
        "        resolved_texts.append(tweet)\n",
        "        resolved_labels.append(most_frequent_label)\n",
        "\n",
        "        # Print specific tweets for verification\n",
        "        if tweet in specific_tweets:\n",
        "            print(f\"Tweet: '{tweet}'\")\n",
        "            print(f\"   Original Labels: {[label_mapping[label] for label in tweet_labels_list]}\")\n",
        "            print(f\"   Assigned Label: {label_mapping[most_frequent_label]}\")\n",
        "\n",
        "    # Step 4: Display results\n",
        "    num_duplicates_resolved = len(tweet_labels) - len(set(texts))\n",
        "\n",
        "    return resolved_texts, resolved_labels\n",
        "\n",
        "# Resolve duplicates for training, validation, and test datasets\n",
        "resolved_train_texts, resolved_train_labels = resolve_duplicates(hf_train['text'], hf_train['label'], \"training\")\n",
        "resolved_val_texts, resolved_val_labels = resolve_duplicates(hf_validation['text'], hf_validation['label'], \"validation\")\n",
        "resolved_test_texts, resolved_test_labels = resolve_duplicates(hf_test['text'], hf_test['label'], \"test\")\n",
        "\n",
        "# Create new datasets with resolved duplicates\n",
        "hf_train = Dataset.from_dict({'text': resolved_train_texts, 'label': resolved_train_labels})\n",
        "hf_validation = Dataset.from_dict({'text': resolved_val_texts, 'label': resolved_val_labels})\n",
        "hf_test = Dataset.from_dict({'text': resolved_test_texts, 'label': resolved_test_labels})\n",
        "\n",
        "print(\"Duplicates resolved in all datasets based on global label frequencies.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2wjN8CedOF"
      },
      "source": [
        "# Count Remaining Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CJuWz63fBpO",
        "outputId": "53577892-7166-4086-8bf0-6a944915de4b"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of each tweet\n",
        "tweet_counts_train = Counter(hf_train['text'])\n",
        "tweet_counts_validation = Counter(hf_validation['text'])\n",
        "tweet_counts_test = Counter(hf_test['text'])\n",
        "\n",
        "# Combine counts from all datasets\n",
        "tweet_counts = tweet_counts_train + tweet_counts_validation + tweet_counts_test\n",
        "\n",
        "# Filter tweets that appear more than once\n",
        "duplicates = [tweet for tweet, count in tweet_counts.items() if count > 1]\n",
        "\n",
        "print(f'Number of duplicates: {len(duplicates)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h_PReO_b2Mv"
      },
      "source": [
        "# Ensure no Overlaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIzLptn6b3u1",
        "outputId": "8184383b-8a04-4ea2-bf74-8c640e6cb422"
      },
      "outputs": [],
      "source": [
        "# Step 2: Check for overlaps between datasets\n",
        "train_texts = set(hf_train['text'])\n",
        "val_texts = set(hf_validation['text'])\n",
        "test_texts = set(hf_test['text'])\n",
        "\n",
        "# Identify overlaps\n",
        "overlap_train_val = train_texts & val_texts\n",
        "overlap_train_test = train_texts & test_texts\n",
        "overlap_val_test = val_texts & test_texts\n",
        "\n",
        "# Display overlaps\n",
        "print(f\"Overlaps between Train and Validation: {len(overlap_train_val)}\")\n",
        "print(f\"Overlaps between Train and Test: {len(overlap_train_test)}\")\n",
        "print(f\"Overlaps between Validation and Test: {len(overlap_val_test)}\")\n",
        "\n",
        "# Remove overlaps\n",
        "hf_validation = {\n",
        "    'text': [text for text in hf_validation['text'] if text not in train_texts and text not in overlap_val_test],\n",
        "    'label': [label for text, label in zip(hf_validation['text'], hf_validation['label']) if text not in train_texts and text not in overlap_val_test]\n",
        "}\n",
        "\n",
        "hf_test = {\n",
        "    'text': [text for text in hf_test['text'] if text not in train_texts and text not in val_texts],\n",
        "    'label': [label for text, label in zip(hf_test['text'], hf_test['label']) if text not in train_texts and text not in val_texts]\n",
        "}\n",
        "\n",
        "# Convert updated validation and test sets back to Dataset objects\n",
        "hf_validation = Dataset.from_dict(hf_validation)\n",
        "hf_test = Dataset.from_dict(hf_test)\n",
        "\n",
        "# Recalculate and print remaining overlaps\n",
        "val_texts = set(hf_validation['text'])\n",
        "test_texts = set(hf_test['text'])\n",
        "\n",
        "print(f\"Remaining overlaps between Train and Validation: {len(train_texts & val_texts)}\")\n",
        "print(f\"Remaining overlaps between Train and Test: {len(train_texts & test_texts)}\")\n",
        "print(f\"Remaining overlaps between Validation and Test: {len(val_texts & test_texts)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1Ie9lmeg-K"
      },
      "source": [
        "# Sanity Check by ensuring no Duplicates are left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edyxtnKFaqD-",
        "outputId": "0cd74f4d-11ac-4cb0-fbd3-821a65d3bb10"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of each tweet\n",
        "tweet_counts_train = Counter(hf_train['text'])\n",
        "tweet_counts_validation = Counter(hf_validation['text'])\n",
        "tweet_counts_test = Counter(hf_test['text'])\n",
        "\n",
        "# Combine counts from all datasets\n",
        "tweet_counts = tweet_counts_train + tweet_counts_validation + tweet_counts_test\n",
        "\n",
        "# Filter tweets that appear more than once\n",
        "duplicates = [tweet for tweet, count in tweet_counts.items() if count > 1]\n",
        "\n",
        "print(f'Number of duplicates: {len(duplicates)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROld1R6mYGL"
      },
      "source": [
        "# Building a Vocabulary/Tokenization and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohbkJwNFmab8",
        "outputId": "8b9bc22d-97ea-4245-e9be-84e0514fbc6a"
      },
      "outputs": [],
      "source": [
        "tokenized_texts = [text.split() for text in train_texts]\n",
        "unique_tokens = set(word for tweet in tokenized_texts for word in tweet)\n",
        "print(len(unique_tokens), \"unique tokens in training set.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln04FK9_epsg"
      },
      "source": [
        "# Checking Number of Unique Tokens (words), Number of Total Tokens, and how many Tokens are needed to cover 95% of the Word Occurences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5knpPZJMml0y",
        "outputId": "5b708ae5-f7db-41a4-8ece-d23680094d61"
      },
      "outputs": [],
      "source": [
        "train_texts = hf_train['text']\n",
        "\n",
        "# 1. Tokenize everything in your training set\n",
        "tokenized_texts = [text.split() for text in train_texts]\n",
        "\n",
        "# 2. Flatten the list of lists to get a single list of all tokens\n",
        "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
        "\n",
        "# 3. Count frequencies\n",
        "freq_counter = Counter(all_tokens)\n",
        "print(f\"Number of unique tokens: {len(freq_counter)}\")\n",
        "\n",
        "# 4. Sort words by descending frequency\n",
        "sorted_token_freqs = freq_counter.most_common()  # list of (word, count), sorted descending\n",
        "total_tokens = sum(freq_counter.values())        # total occurrences of all tokens\n",
        "\n",
        "print(f\"Total token occurrences in train set: {total_tokens}\")\n",
        "\n",
        "# 5. Compute cumulative coverage\n",
        "cumulative = 0\n",
        "coverage_break = 0.95  # or 0.90, 0.98, etc.\n",
        "needed_words = 0\n",
        "\n",
        "for i, (token, count) in enumerate(sorted_token_freqs, start=1):\n",
        "    cumulative += count\n",
        "    if (cumulative / total_tokens) >= coverage_break:\n",
        "        needed_words = i\n",
        "        break\n",
        "\n",
        "print(f\"To cover {coverage_break*100}% of word occurrences, you need ~{needed_words} words.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dgdkzl_e9VB"
      },
      "source": [
        "# Defining the Train, Validation, and Test Texts (Tweets) and the Train, Validationa and Test Labels (Classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULp2yyZ9qHau",
        "outputId": "186ce881-3ff8-4a16-ed51-54460baa05fb"
      },
      "outputs": [],
      "source": [
        "train_texts = hf_train['text']\n",
        "val_texts   = hf_validation['text']\n",
        "test_texts  = hf_test['text']\n",
        "\n",
        "y_train = hf_train['label']\n",
        "y_val   = hf_validation['label']\n",
        "y_test  = hf_test['label']\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "print(len(test_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRufL2JTfKnj"
      },
      "source": [
        "# Creating the Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIKvDZ_cq1jP",
        "outputId": "4ec5e6dd-a90f-4ecc-c831-2b492f50c9d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "MAX_VOCAB_SIZE = 6000\n",
        "\n",
        "encoder = TextVectorization(\n",
        "    max_tokens=MAX_VOCAB_SIZE,\n",
        "    output_mode='int'\n",
        ")\n",
        "\n",
        "encoder.adapt(train_texts)  # Build vocabulary from training texts\n",
        "\n",
        "\n",
        "# 2. Inspect the Vocabulary\n",
        "\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "print(f\"First 50 words in the vocabulary:\\n{vocab[:50]}\\n\")\n",
        "print(f\"Vocabulary length: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPENtmI0q48J",
        "outputId": "7e1da21c-b2fe-4052-a15a-2701eae1ab07"
      },
      "outputs": [],
      "source": [
        "# Encode each split separately\n",
        "encoded_train = encoder(train_texts).numpy()\n",
        "encoded_val   = encoder(val_texts).numpy()\n",
        "encoded_test  = encoder(test_texts).numpy()\n",
        "\n",
        "print(\"Example of encoded train data:\", encoded_train[0])\n",
        "print(\"Shape of encoded train:\", encoded_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Jkier1fQjP"
      },
      "source": [
        "# Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rJcWn9eq7-e",
        "outputId": "7efd5f55-e57d-4024-d6d0-61d2dda45cc3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 50\n",
        "\n",
        "X_train = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating='post')\n",
        "X_val   = pad_sequences(encoded_val, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test  = pad_sequences(encoded_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "print(hf_train[0])\n",
        "print(\"Example of encoded train data:\", X_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkruvArJseBL"
      },
      "source": [
        "# Standard RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JboxvCazIQqz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "p1g24CCXrptU",
        "outputId": "03a4c8c6-4bbd-43ef-ef83-3931c362e33e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "Standard_RNN_model = Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,  # must match or exceed your vocabulary size\n",
        "        output_dim=128,            # embedding vector size\n",
        "        input_length=max_length,   # the same length you used in pad_sequences\n",
        "        mask_zero=True             # optional: treat 0 as padding\n",
        "    ),\n",
        "\n",
        "    layers.SimpleRNN(128, dropout=0.2),\n",
        "\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "Standard_RNN_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build the model explicitly with input shape\n",
        "Standard_RNN_model.build(input_shape=(None, max_length))\n",
        "\n",
        "Standard_RNN_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGvOJf-lsqyF"
      },
      "source": [
        "# Standard GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "whVQbocFsswJ",
        "outputId": "90a0c846-e4eb-4eab-884e-a4591b411821"
      },
      "outputs": [],
      "source": [
        "Standard_GRU_model = Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,  # must match or exceed your vocabulary size\n",
        "        output_dim=128,            # embedding vector size\n",
        "        input_length=max_length,   # the same length you used in pad_sequences\n",
        "        mask_zero=True             # optional: treat 0 as padding\n",
        "    ),\n",
        "\n",
        "    layers.GRU(128, dropout=0.2),\n",
        "\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "Standard_GRU_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build the model explicitly with input shape\n",
        "Standard_GRU_model.build(input_shape=(None, max_length))\n",
        "\n",
        "Standard_GRU_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rgF6kFs2mW"
      },
      "source": [
        "# Standard LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "UCJcyoRHs368",
        "outputId": "c6aab18d-7fca-43ad-cf18-9fedc6f5c1f7"
      },
      "outputs": [],
      "source": [
        "Standard_LSTM_model = Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,  # must match or exceed your vocabulary size\n",
        "        output_dim=128,            # embedding vector size\n",
        "        input_length=max_length,   # the same length you used in pad_sequences\n",
        "        mask_zero=True             # optional: treat 0 as padding\n",
        "    ),\n",
        "\n",
        "    layers.LSTM(128, dropout=0.2),\n",
        "\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "Standard_LSTM_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build the model explicitly with input shape\n",
        "Standard_LSTM_model.build(input_shape=(None, max_length))\n",
        "\n",
        "Standard_LSTM_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9fuhVhStF01"
      },
      "source": [
        "# Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "aiO7lAkftKwb",
        "outputId": "d3a219ab-1480-43dd-8ce2-f7c6868f3372"
      },
      "outputs": [],
      "source": [
        "Bidirectional_RNN_model = Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,  # must match or exceed your vocabulary size\n",
        "        output_dim=128,            # embedding vector size\n",
        "        input_length=max_length,   # the same length you used in pad_sequences\n",
        "        mask_zero=True             # optional: treat 0 as padding\n",
        "    ),\n",
        "\n",
        "    layers.Bidirectional(layers.SimpleRNN(128,\n",
        "                                          dropout=0.2),\n",
        "    ),\n",
        "\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "Bidirectional_RNN_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build the model explicitly with input shape\n",
        "Bidirectional_RNN_model.build(input_shape=(None, max_length))\n",
        "\n",
        "Bidirectional_RNN_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8uGqdmGtbYy"
      },
      "source": [
        "# Bidirectioanl GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "hp0M2rvHtdCM",
        "outputId": "02b35250-2709-47c0-fade-1b1c0dfa0004"
      },
      "outputs": [],
      "source": [
        "Bidirectional_GRU_model = Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,  # must match or exceed your vocabulary size\n",
        "        output_dim=128,            # embedding vector size\n",
        "        input_length=max_length,   # the same length you used in pad_sequences\n",
        "        mask_zero=True             # optional: treat 0 as padding\n",
        "    ),\n",
        "    layers.Bidirectional(\n",
        "        layers.GRU(128,\n",
        "                   dropout=0.2)\n",
        "    ),\n",
        "    layers.Dense(6,\n",
        "                 activation='softmax')  # 6 emotion classes\n",
        "])\n",
        "\n",
        "Bidirectional_GRU_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',  # if labels are int-coded\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build the model explicitly with input shape\n",
        "Bidirectional_GRU_model.build(input_shape=(None, max_length))\n",
        "\n",
        "Bidirectional_GRU_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccMgCWZouqf-"
      },
      "source": [
        "# Bidirectioanl LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "DUlovofrupbU",
        "outputId": "2c69dc58-51d0-461e-a410-cd10ab82140b"
      },
      "outputs": [],
      "source": [
        "Bidirectional_LSTM_model = Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,  # must match or exceed your vocabulary size\n",
        "        output_dim=128,            # embedding vector size\n",
        "        input_length=max_length,   # the same length you used in pad_sequences\n",
        "        mask_zero=True             # optional: treat 0 as padding\n",
        "    ),\n",
        "\n",
        "    layers.Bidirectional(layers.LSTM(128,\n",
        "                                          dropout=0.2),\n",
        "    ),\n",
        "\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "Bidirectional_LSTM_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build the model explicitly with input shape\n",
        "Bidirectional_LSTM_model.build(input_shape=(None, max_length))\n",
        "\n",
        "Bidirectional_LSTM_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5cLx1vvKGn"
      },
      "source": [
        "# Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ0C3YxmvO0t"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vaGzsjtvXrn"
      },
      "source": [
        "# Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP1q3zy8vZak",
        "outputId": "7fb40ff7-7399-4ea9-c433-cce6b72c72e8"
      },
      "outputs": [],
      "source": [
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "weights = dict(enumerate(class_weights_array))\n",
        "print(weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riv4Kbqqu8AB"
      },
      "source": [
        "# Fitting the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXLaN0wau9QO",
        "outputId": "45925d13-7203-409f-995f-78d746d38edb"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype('int32')\n",
        "X_val   = X_val.astype('int32')\n",
        "X_test  = X_test.astype('int32')\n",
        "\n",
        "print(X_train.dtype)  # int32\n",
        "print(X_train.shape)  # (num_samples, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VCq8_wxvo_1",
        "outputId": "114a80e8-cf45-48e0-daaf-8b58d5bdc03d"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train, dtype='int32')\n",
        "y_val = np.array(y_val, dtype='int32')\n",
        "y_test = np.array(y_test, dtype='int32')\n",
        "\n",
        "print(y_train.dtype)  # int32\n",
        "print(y_train.shape)  # (num_samples,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RThaZBvOvsN0",
        "outputId": "3104b892-aa44-4683-8450-82d16cdbb87f"
      },
      "outputs": [],
      "source": [
        "# Standard RNN\n",
        "history = Standard_RNN_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfzldp1Rv2Fi",
        "outputId": "f9226641-5995-4511-c077-a026fa60d3f8"
      },
      "outputs": [],
      "source": [
        "# Bidirectional RNN\n",
        "history = Bidirectional_RNN_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwYYojIPwPPw",
        "outputId": "a50c375a-4466-4788-c92f-e935ce5df41d"
      },
      "outputs": [],
      "source": [
        "# Standard GRU\n",
        "history = Standard_GRU_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtQdzS94wS28",
        "outputId": "ecf16701-6a9f-425a-bf26-40fdd00d4208"
      },
      "outputs": [],
      "source": [
        "# Bidirectional GRU\n",
        "history = Bidirectional_GRU_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUz6n5SvwaJG",
        "outputId": "6b637101-58a7-462b-8ac5-07ba17660e60"
      },
      "outputs": [],
      "source": [
        "# Standard LSTM\n",
        "history = Standard_LSTM_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G_cWMWKwhHK",
        "outputId": "5ed28412-14f2-4011-b1e3-5f934952bcde"
      },
      "outputs": [],
      "source": [
        "# Bidirectional LSTM\n",
        "history = Bidirectional_LSTM_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARfM3zJU3NOp"
      },
      "source": [
        "# Test Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM3ittpj3Ow0",
        "outputId": "5db567f3-c94d-42ff-90b5-07d66d0ea101"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Standard RNN\n",
        "rnn_predictions = Standard_RNN_model.predict(X_test)\n",
        "rnn_predicted_labels = rnn_predictions.argmax(axis=1)\n",
        "standard_rnn_accuracy = accuracy_score(y_test, rnn_predicted_labels)\n",
        "\n",
        "# Bidirectional RNN\n",
        "bidir_rnn_predictions = Bidirectional_RNN_model.predict(X_test)\n",
        "bidir_rnn_predicted_labels = bidir_rnn_predictions.argmax(axis=1)\n",
        "bidirectional_rnn_accuracy = accuracy_score(y_test, bidir_rnn_predicted_labels)\n",
        "\n",
        "# Standard GRU\n",
        "gru_predictions = Standard_GRU_model.predict(X_test)\n",
        "gru_predicted_labels = gru_predictions.argmax(axis=1)\n",
        "standard_gru_accuracy = accuracy_score(y_test, gru_predicted_labels)\n",
        "\n",
        "# Bidirectional GRU\n",
        "bidir_gru_predictions = Bidirectional_GRU_model.predict(X_test)\n",
        "bidir_gru_predicted_labels = bidir_gru_predictions.argmax(axis=1)\n",
        "bidirectional_gru_accuracy = accuracy_score(y_test, bidir_gru_predicted_labels)\n",
        "\n",
        "# Standard LSTM\n",
        "lstm_predictions = Standard_LSTM_model.predict(X_test)\n",
        "lstm_predicted_labels = lstm_predictions.argmax(axis=1)\n",
        "standard_lstm_accuracy = accuracy_score(y_test, lstm_predicted_labels)\n",
        "\n",
        "# Bidirectional LSTM\n",
        "bidir_lstm_predictions = Bidirectional_LSTM_model.predict(X_test)\n",
        "bidir_lstm_predicted_labels = bidir_lstm_predictions.argmax(axis=1)\n",
        "bidirectional_lstm_accuracy = accuracy_score(y_test, bidir_lstm_predicted_labels)\n",
        "\n",
        "\n",
        "print(f\"Standard RNN Test Accuracy: {standard_rnn_accuracy:.4f}\")\n",
        "print(f\"Bidirectional RNN Test Accuracy: {bidirectional_rnn_accuracy:.4f}\")\n",
        "print(f\"Standard GRU Test Accuracy: {standard_gru_accuracy:.4f}\")\n",
        "print(f\"Bidirectional GRU Test Accuracy: {bidirectional_gru_accuracy:.4f}\")\n",
        "print(f\"Standard LSTM Test Accuracy: {standard_lstm_accuracy:.4f}\")\n",
        "print(f\"Bidirectional LSTM Test Accuracy: {bidirectional_lstm_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3UWeJ9t5Mhl"
      },
      "source": [
        "# Classification Reports on the Test Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra1wAVYf5Pbz",
        "outputId": "a1de47c7-46f8-4421-954f-1821b0425bfc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Standard RNN\n",
        "test_predictions_rnn = Standard_RNN_model.predict(X_test)\n",
        "test_predicted_labels_rnn = test_predictions_rnn.argmax(axis=1)\n",
        "test_classification_report_rnn = classification_report(y_test, test_predicted_labels_rnn, target_names=label_mapping.values())\n",
        "\n",
        "# Bidirectional RNN\n",
        "test_predictions_bidir_rnn = Bidirectional_RNN_model.predict(X_test)\n",
        "test_predicted_labels_bidir_rnn = test_predictions_bidir_rnn.argmax(axis=1)\n",
        "test_classification_report_bidir_rnn = classification_report(y_test, test_predicted_labels_bidir_rnn, target_names=label_mapping.values())\n",
        "\n",
        "# Standard GRU\n",
        "test_predictions_gru = Standard_GRU_model.predict(X_test)\n",
        "test_predicted_labels_gru = test_predictions_gru.argmax(axis=1)\n",
        "test_classification_report_gru = classification_report(y_test, test_predicted_labels_gru, target_names=label_mapping.values())\n",
        "\n",
        "# Bidirectional GRU\n",
        "test_predictions_bidir_gru = Bidirectional_GRU_model.predict(X_test)\n",
        "test_predicted_labels_bidir_gru = test_predictions_bidir_gru.argmax(axis=1)\n",
        "test_classification_report_bidir_gru = classification_report(y_test, test_predicted_labels_bidir_gru, target_names=label_mapping.values())\n",
        "\n",
        "# Standard LSTM\n",
        "test_predictions_lstm = Standard_LSTM_model.predict(X_test)\n",
        "test_predicted_labels_lstm = test_predictions_lstm.argmax(axis=1)\n",
        "test_classification_report_lstm = classification_report(y_test, test_predicted_labels_lstm, target_names=label_mapping.values())\n",
        "\n",
        "# Bidirectional LSTM\n",
        "test_predictions_bidir_lstm = Bidirectional_LSTM_model.predict(X_test)\n",
        "test_predicted_labels_bidir_lstm = test_predictions_bidir_lstm.argmax(axis=1)\n",
        "test_classification_report_bidir_lstm = classification_report(y_test, test_predicted_labels_bidir_lstm, target_names=label_mapping.values())\n",
        "\n",
        "print(\"Classification Report for Standard RNN:\")\n",
        "print(test_classification_report_rnn)\n",
        "\n",
        "print(\"Classification Report for Bidirectional RNN:\")\n",
        "print(test_classification_report_bidir_rnn)\n",
        "\n",
        "print(\"Classification Report for Standard GRU:\")\n",
        "print(test_classification_report_gru)\n",
        "\n",
        "print(\"Classification Report for Bidirectional GRU:\")\n",
        "print(test_classification_report_bidir_gru)\n",
        "\n",
        "print(\"Classification Report for Standard LSTM:\")\n",
        "print(test_classification_report_lstm)\n",
        "\n",
        "print(\"Classification Report for Bidirectional LSTM:\")\n",
        "print(test_classification_report_bidir_lstm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMotOEIc7BQ5"
      },
      "source": [
        "# Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DLsAUrwQ7awJ",
        "outputId": "b97d1a15-4337-4c13-b9da-a330a1e65a87"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "heatmap_colors = {\n",
        "    \"Standard RNN\": \"Blues\",\n",
        "    \"Bidirectional RNN\": \"Blues\",\n",
        "    \"Standard GRU\": \"Greens\",\n",
        "    \"Bidirectional GRU\": \"Greens\",\n",
        "    \"Standard LSTM\": \"Reds\",\n",
        "    \"Bidirectional LSTM\": \"Reds\"\n",
        "}\n",
        "\n",
        "\n",
        "# Standard RNN\n",
        "cm_rnn = confusion_matrix(y_test, test_predicted_labels_rnn)\n",
        "sns.heatmap(cm_rnn, annot=True, fmt='d', cmap=heatmap_colors[\"Standard RNN\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Standard RNN')\n",
        "plt.show()\n",
        "\n",
        "# Bidirectional RNN\n",
        "cm_bidir_rnn = confusion_matrix(y_test, test_predicted_labels_bidir_rnn)\n",
        "sns.heatmap(cm_bidir_rnn, annot=True, fmt='d', cmap=heatmap_colors[\"Bidirectional RNN\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Bidirectional RNN')\n",
        "plt.show()\n",
        "\n",
        "# Standard GRU\n",
        "cm_gru = confusion_matrix(y_test, test_predicted_labels_gru)\n",
        "sns.heatmap(cm_gru, annot=True, fmt='d', cmap=heatmap_colors[\"Standard GRU\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Standard GRU')\n",
        "plt.show()\n",
        "\n",
        "# Bidirectional GRU\n",
        "cm_bidir_gru = confusion_matrix(y_test, test_predicted_labels_bidir_gru)\n",
        "sns.heatmap(cm_bidir_gru, annot=True, fmt='d', cmap=heatmap_colors[\"Bidirectional GRU\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Bidirectional GRU')\n",
        "plt.show()\n",
        "\n",
        "# Standard LSTM\n",
        "cm_lstm = confusion_matrix(y_test, test_predicted_labels_lstm)\n",
        "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap=heatmap_colors[\"Standard LSTM\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Standard LSTM')\n",
        "plt.show()\n",
        "\n",
        "# Bidirectional LSTM\n",
        "cm_bidir_lstm = confusion_matrix(y_test, test_predicted_labels_bidir_lstm)\n",
        "sns.heatmap(cm_bidir_lstm, annot=True, fmt='d', cmap=heatmap_colors[\"Bidirectional LSTM\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Bidirectional LSTM')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2McVXHXASx02"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPWqH4yYS2pd"
      },
      "source": [
        "## Looking at the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5TAVb2xS4LV",
        "outputId": "e7e695d0-8cc1-48d6-dad6-76aa2315f224"
      },
      "outputs": [],
      "source": [
        "# View the first 5 tweets and their mapped labels\n",
        "for i in range(15):\n",
        "    print(f\"Tweet: {hf_train['text'][i]}\")\n",
        "    print(f\"Label: {label_mapping[hf_train['label'][i]]}\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71d3rYlPS8_K"
      },
      "source": [
        "## Checking if everything is lowercased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUZz_lzDTAcQ",
        "outputId": "0fdc84d4-3a5f-4194-cc0c-d80770d85e92"
      },
      "outputs": [],
      "source": [
        "# Check for uppercase letters in hf_train\n",
        "contains_uppercase = False  # Reset flag for hf_train\n",
        "for i, tweet in enumerate(hf_train['text']):\n",
        "    if any(char.isupper() for char in tweet):  # Check if any character is uppercase\n",
        "        contains_uppercase = True\n",
        "        print(f\"Tweet {i} contains uppercase letters in hf_train: {tweet}\")\n",
        "\n",
        "if not contains_uppercase:\n",
        "    print(\"No tweets contain uppercase letters in hf_train\")\n",
        "\n",
        "# Check for uppercase letters in hf_validation\n",
        "contains_uppercase = False  # Reset flag for hf_validation\n",
        "for i, tweet in enumerate(hf_validation['text']):\n",
        "    if any(char.isupper() for char in tweet):  # Check if any character is uppercase\n",
        "        contains_uppercase = True\n",
        "        print(f\"Tweet {i} contains uppercase letters in hf_validation: {tweet}\")\n",
        "\n",
        "if not contains_uppercase:\n",
        "    print(\"No tweets contain uppercase letters in hf_validation\")\n",
        "\n",
        "# Check for uppercase letters in hf_test\n",
        "contains_uppercase = False  # Reset flag for hf_test\n",
        "for i, tweet in enumerate(hf_test['text']):\n",
        "    if any(char.isupper() for char in tweet):  # Check if any character is uppercase\n",
        "        contains_uppercase = True\n",
        "        print(f\"Tweet {i} contains uppercase letters in hf_test: {tweet}\")\n",
        "\n",
        "if not contains_uppercase:\n",
        "    print(\"No tweets contain uppercase letters in hf_test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CihZxtJpT4dn"
      },
      "source": [
        "## Checking for Emojies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlZIB_-jT6Fy",
        "outputId": "d65bc81d-d791-4cd6-ac17-d1b8dbb9f66c"
      },
      "outputs": [],
      "source": [
        "def contains_emoji(text):\n",
        "    # Regular expression to match emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
        "        u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
        "        u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE,\n",
        "    )\n",
        "    return bool(emoji_pattern.search(text))\n",
        "\n",
        "# Check for emojis in the training set\n",
        "emoji_in_train = any(contains_emoji(text) for text in hf_train['text'])\n",
        "\n",
        "# Check for emojis in validation and test sets (optional)\n",
        "emoji_in_validation = any(contains_emoji(text) for text in hf_validation['text'])\n",
        "emoji_in_test = any(contains_emoji(text) for text in hf_test['text'])\n",
        "\n",
        "# Print results\n",
        "print(f\"Emojis in training set: {emoji_in_train}\")\n",
        "print(f\"Emojis in validation set: {emoji_in_validation}\")\n",
        "print(f\"Emojis in test set: {emoji_in_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRWHwK_ET9Tg"
      },
      "source": [
        "## Looking at Misspelled Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1KS1-xtUEB_"
      },
      "source": [
        "## Checking for Misspelled Words in the Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f4NMt3PUCv9",
        "outputId": "b9e874dc-fccf-48bf-c2b9-d46b85124e49"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Initialize the spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Counter for the total number of misspelled words\n",
        "total_misspelled_words = 0\n",
        "\n",
        "# Counter to track how many tweets with misspelled words have been printed\n",
        "misspelled_count = 0\n",
        "\n",
        "# Check for misspelled words in tweets\n",
        "for i, tweet in enumerate(hf_train['text']):\n",
        "    # Tokenize the tweet into words\n",
        "    words = tweet.split()\n",
        "    # Find misspelled words\n",
        "    misspelled = spell.unknown(words)\n",
        "\n",
        "    # Count the total misspelled words\n",
        "    total_misspelled_words += len(misspelled)\n",
        "\n",
        "    # Print the first 10 tweets with misspelled words\n",
        "    if misspelled and misspelled_count < 10:\n",
        "        print(f\"Tweet {i} contains misspelled words: {tweet}\")\n",
        "        print(f\"   Misspelled words: {', '.join(misspelled)}\")\n",
        "        misspelled_count += 1\n",
        "\n",
        "# Print the total number of misspelled words\n",
        "print(f\"Total number of misspelled words in hf_validation: {total_misspelled_words}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQkLykNRVkhj"
      },
      "source": [
        "## Checking for Misspelled Words in the Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdlnGdamU0hM",
        "outputId": "f4f0990f-c9c7-4ed8-beaa-92268867987d"
      },
      "outputs": [],
      "source": [
        "# Initialize the spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Counter for the total number of misspelled words\n",
        "total_misspelled_words = 0\n",
        "\n",
        "# Counter to track how many tweets with misspelled words have been printed\n",
        "misspelled_count = 0\n",
        "\n",
        "# Check for misspelled words in tweets\n",
        "for i, tweet in enumerate(hf_validation['text']):\n",
        "    # Tokenize the tweet into words\n",
        "    words = tweet.split()\n",
        "    # Find misspelled words\n",
        "    misspelled = spell.unknown(words)\n",
        "\n",
        "    # Count the total misspelled words\n",
        "    total_misspelled_words += len(misspelled)\n",
        "\n",
        "    # Print the first 5 tweets with misspelled words\n",
        "    if misspelled and misspelled_count < 5:\n",
        "        print(f\"Tweet {i} contains misspelled words: {tweet}\")\n",
        "        print(f\"   Misspelled words: {', '.join(misspelled)}\")\n",
        "        misspelled_count += 1\n",
        "\n",
        "# Print the total number of misspelled words\n",
        "print(f\"Total number of misspelled words in hf_validation: {total_misspelled_words}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2kDzlrBVm-4"
      },
      "source": [
        "## Checking for Misspelled Words in the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e_PwjBYU_5z",
        "outputId": "134ccf44-b6a0-4d81-8f7f-6be84c0f84d1"
      },
      "outputs": [],
      "source": [
        "# Initialize the spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Counter for the total number of misspelled words\n",
        "total_misspelled_words = 0\n",
        "\n",
        "# Counter to track how many tweets with misspelled words have been printed\n",
        "misspelled_count = 0\n",
        "\n",
        "# Check for misspelled words in tweets\n",
        "for i, tweet in enumerate(hf_test['text']):\n",
        "    # Tokenize the tweet into words\n",
        "    words = tweet.split()\n",
        "    # Find misspelled words\n",
        "    misspelled = spell.unknown(words)\n",
        "\n",
        "    # Count the total misspelled words\n",
        "    total_misspelled_words += len(misspelled)\n",
        "\n",
        "    # Print the first 5 tweets with misspelled words\n",
        "    if misspelled and misspelled_count < 5:\n",
        "        print(f\"Tweet {i} contains misspelled words: {tweet}\")\n",
        "        print(f\"   Misspelled words: {', '.join(misspelled)}\")\n",
        "        misspelled_count += 1\n",
        "\n",
        "# Print the total number of misspelled words\n",
        "print(f\"Total number of misspelled words in hf_test: {total_misspelled_words}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4WovWfLW3ZM"
      },
      "source": [
        "## Correcting Common Misspelled Words, i.e. Contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVALfA5CW63T",
        "outputId": "7104abd1-4e3a-4345-fb99-bec154c2b144"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary of common contractions expanded to full forms\n",
        "contraction_mapping = {\n",
        "    \"i m\": \"i am\",\n",
        "    \"can t\": \"cannot\",\n",
        "    \"don t\": \"do not\",\n",
        "    \"doesn t\": \"does not\",\n",
        "    \"didn t\": \"did not\",\n",
        "    \"won t\": \"will not\",\n",
        "    \"couldn t\": \"could not\",\n",
        "    \"shouldn t\": \"should not\",\n",
        "    \"wouldn t\": \"would not\",\n",
        "    \"haven t\": \"have not\",\n",
        "    \"hasn t\": \"has not\",\n",
        "    \"aren t\": \"are not\",\n",
        "    \"isn t\": \"is not\",\n",
        "    \"wasn t\": \"was not\",\n",
        "    \"weren t\": \"were not\",\n",
        "    \"they re\": \"they are\",\n",
        "    \"we re\": \"we are\",\n",
        "    \"you re\": \"you are\",\n",
        "    \"it s\": \"it is\",\n",
        "    \"that s\": \"that is\",\n",
        "    \"what s\": \"what is\",\n",
        "    \"who s\": \"who is\",\n",
        "    \"how s\": \"how is\",\n",
        "    \"im\": \"i am\",\n",
        "    \"dont\": \"do not\",\n",
        "    \"cant\": \"cannot\",\n",
        "    \"didnt\": \"did not\",\n",
        "    \"wasnt\": \"was not\",\n",
        "    \"werent\": \"were not\",\n",
        "    \"shouldnt\": \"should not\",\n",
        "    \"couldnt\": \"could not\",\n",
        "    \"wouldnt\": \"would not\",\n",
        "    \"hasnt\": \"has not\",\n",
        "    \"havent\": \"have not\",\n",
        "    \"arent\": \"are not\",\n",
        "    \"isnt\": \"is not\",\n",
        "    \"wont\": \"will not\",\n",
        "    \"doesnt\": \"does not\",\n",
        "    \"hadnt\": \"had not\",\n",
        "    \"theyre\": \"they are\",\n",
        "    \"youre\": \"you are\",\n",
        "    \"thats\": \"that is\",\n",
        "    \"heres\": \"here is\",\n",
        "    \"whats\": \"what is\",\n",
        "    \"whos\": \"who is\",\n",
        "    \"hows\": \"how is\",\n",
        "    \"theres\": \"there is\",\n",
        "    \"lets\": \"let us\",\n",
        "    \"ive\": \"i have\",\n",
        "    \"youve\": \"you have\",\n",
        "    \"weve\": \"we have\",\n",
        "    \"theyve\": \"they have\",\n",
        "    \"youd\": \"you would\",\n",
        "    \"hed\": \"he would\",\n",
        "    \"theyd\": \"they would\",\n",
        "    \"youll\": \"you will\",\n",
        "    \"theyll\": \"they will\",\n",
        "    \"idve\": \"i would have\",\n",
        "    \"hedve\": \"he would have\",\n",
        "    \"shedve\": \"she would have\",\n",
        "    \"wedve\": \"we would have\",\n",
        "    \"theydve\": \"they would have\",\n",
        "}\n",
        "\n",
        "# Apply normalization to the dataset\n",
        "def fix_contractions(example):\n",
        "    # Ensure contraction mapping only replaces standalone words\n",
        "    contraction_pattern = re.compile(\n",
        "        r'\\b(' + '|'.join(re.escape(key) for key in contraction_mapping.keys()) + r')\\b', flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    def replace_match(match):\n",
        "        # Replace based on the contraction mapping\n",
        "        contraction = match.group(0).lower()  # Match is case-insensitive\n",
        "        return contraction_mapping.get(contraction, contraction)\n",
        "\n",
        "    # Replace contractions in the text\n",
        "    example['text'] = contraction_pattern.sub(replace_match, example['text'])\n",
        "    return example\n",
        "\n",
        "# Example testing function\n",
        "def test_fix_contractions():\n",
        "    tweet = \"i m cleverly got him to the rail quietly i was feeling pretty clever and climbed up and got on, don t dontestanly immersive. theydve\"\n",
        "\n",
        "    example = {\"text\": tweet}\n",
        "    fixed_example = fix_contractions(example)\n",
        "    return fixed_example\n",
        "\n",
        "# Test the function\n",
        "result = test_fix_contractions()\n",
        "print(result['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8irSoz_fpE2"
      },
      "source": [
        "# Correcting Concrations in the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265,
          "referenced_widgets": [
            "652a21682c6a492d802a5e35a5263921",
            "d6097e4d5b7740f8812177f4a8519acd",
            "51d874efcf5a4f02a8070a90cac8fce8",
            "2ae73fdfe6514a039011ee5e64661b0f",
            "0ab0682e2cbf44549a2c42944e6aec2e",
            "eb6215d21e224998be36401e69436ae1",
            "2a795317f1f44175add0fbb8911888ff",
            "1d1533e227b544769ad2c220758febd8",
            "3c598d22102548eeb5d2f5e91c9e69a8",
            "965e03b85d82450aa9819b77dcee33ec",
            "ae920dd0bd984ea6a988381244e97c22"
          ]
        },
        "id": "zX7ZpObaXEQI",
        "outputId": "6bacfb26-9740-433e-bb7c-05a3537acef4"
      },
      "outputs": [],
      "source": [
        "hf_train = hf_train.map(lambda example: fix_contractions(example))\n",
        "\n",
        "# Print a sample of fixed tweets to verify\n",
        "print(\"Sample of fixed tweets:\")\n",
        "for i, tweet in enumerate(hf_train['text'][:10]):  # Show the first 10 tweets\n",
        "    print(f\"Tweet {i}: {tweet}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6kgXW_rfxJP"
      },
      "source": [
        "# Checking Number of Misspelled Words again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3TWrVYsXHX0",
        "outputId": "854ed8e0-8608-4ca4-a4e1-6c15236994e4"
      },
      "outputs": [],
      "source": [
        "# Initialize the spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Counter for the total number of misspelled words\n",
        "total_misspelled_words = 0\n",
        "\n",
        "# Counter to track how many tweets with misspelled words have been printed\n",
        "misspelled_count = 0\n",
        "\n",
        "# Check for misspelled words in tweets\n",
        "for i, tweet in enumerate(hf_train['text']):\n",
        "    # Tokenize the tweet into words\n",
        "    words = tweet.split()\n",
        "\n",
        "    # Find misspelled words\n",
        "    misspelled = spell.unknown(words)\n",
        "\n",
        "    # Count the total misspelled words\n",
        "    total_misspelled_words += len(misspelled)\n",
        "\n",
        "    # Print the first 5 tweets with misspelled words\n",
        "    if misspelled and misspelled_count < 5:\n",
        "        print(f\"Tweet {i} contains misspelled words: {tweet}\")\n",
        "        print(f\"   Misspelled words: {', '.join(misspelled)}\")\n",
        "        misspelled_count += 1\n",
        "\n",
        "# Print the total number of misspelled words\n",
        "print(f\"Total number of misspelled words across all tweets: {total_misspelled_words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2CTe9b4XuW5"
      },
      "source": [
        "# Remove or Normalize URLs, Mentions, and Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c2b7ae00415445e0aba57b3fdf2fddb5",
            "7e1f8343b2d649b59881c164c004f1ec",
            "d91d200d7b6b44c89723da7e0f02c83b",
            "cefb8ff0139243de98eb6615a67bdca4",
            "0e58a4178f1d42a2992e60dfe8e2663d",
            "e76f0daf94da49c9b6faa0ddd489053c",
            "7f744c3cafdb4fa9b9af51875bcd27f6",
            "857969b4464a4772b8d8bb6b5471fe55",
            "a9fce9e2a43949b2a2ecc23d2302d911",
            "5bba9faa56e94e8498737560513aecc3",
            "5f0d4be4bf684c74af2b9039c68d75bf"
          ]
        },
        "id": "hM_mnRGlXt2k",
        "outputId": "8a0226c1-8fa8-4163-945c-a16613dd9e1d"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_fn(examples):\n",
        "    # `examples[\"text\"]` is a list of text strings if batched=True\n",
        "    processed_texts = []\n",
        "    for text in examples[\"text\"]:\n",
        "        # 1. Remove URLs\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "        # 2. Remove mentions\n",
        "        text = re.sub(r\"@\\w+\", '', text)\n",
        "        # 3. Remove hashtags (keep the word)\n",
        "        text = re.sub(r\"#\", '', text)\n",
        "        # 4. Lowercase\n",
        "        text = text.lower()\n",
        "        # 5. Remove extra spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        processed_texts.append(text)\n",
        "\n",
        "    # Return a dict with the updated \"text\" field\n",
        "    return {\"text\": processed_texts}\n",
        "\n",
        "# Apply to each split with `batched=True`\n",
        "hf_train = hf_train.map(preprocess_text_fn, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uE5U92wYB_Z"
      },
      "source": [
        "# Looking at very Short Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOwJxMHXYERd",
        "outputId": "6447841b-9aad-4e90-d8b5-a454721d2317"
      },
      "outputs": [],
      "source": [
        "# Find very short tweets (less than 3 words)\n",
        "short_tweets_train = [tweet for tweet in hf_train['text'] if len(tweet.split()) < 3]\n",
        "short_tweets_validation = [tweet for tweet in hf_validation['text'] if len(tweet.split()) < 3]\n",
        "short_tweets_test = [tweet for tweet in hf_test['text'] if len(tweet.split()) < 3]\n",
        "\n",
        "# Count and display\n",
        "print(f\"Number of very short tweets in hf_train: {len(short_tweets_train)}\")\n",
        "print(\"Examples of very short tweets:\", short_tweets_train[:10])\n",
        "print(f\"Number of very short tweets in hf_validation: {len(short_tweets_validation)}\")\n",
        "print(f\"Number of very short tweets in hf_test: {len(short_tweets_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpB36mGLYskq"
      },
      "source": [
        "## Removing Short Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayPL6AZsYu6B",
        "outputId": "b034967f-5661-4888-c70c-4a2f016251c1"
      },
      "outputs": [],
      "source": [
        "# Filter out short tweets from hf_train\n",
        "filtered_texts = [tweet for tweet in hf_train['text'] if len(tweet.split()) >= 3]\n",
        "filtered_labels = [label for tweet, label in zip(hf_train['text'], hf_train['label']) if len(tweet.split()) >= 3]\n",
        "\n",
        "# Update hf_train with the filtered data\n",
        "hf_train = {'text': filtered_texts, 'label': filtered_labels}\n",
        "\n",
        "short_tweets_train = [tweet for tweet in hf_train['text'] if len(tweet.split()) < 3]\n",
        "print(f\"Number of very short tweets in hf_train: {len(short_tweets_train)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwFU-Kd4YII4"
      },
      "source": [
        "# Looking for Very Long tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB3adhDaYIvJ",
        "outputId": "eb0892d5-0fbd-4643-bd8f-e4d7f2a12cd8"
      },
      "outputs": [],
      "source": [
        "# Find very short tweets (less than 3 words)\n",
        "long_tweets = [tweet for tweet in hf_train['text'] if len(tweet.split()) > 50]\n",
        "\n",
        "# Count and display\n",
        "print(f\"Number of very long tweets: {len(long_tweets)}\")\n",
        "print(\"Examples of very long tweets:\", long_tweets[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jAMuuQNaBlF"
      },
      "source": [
        "# Truncating very long tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-vV5RBRaBE0",
        "outputId": "64d6f7be-adfe-4381-cc5e-1b5537530fa7"
      },
      "outputs": [],
      "source": [
        "def truncate_long_tweets(tweet, max_words=50, keep_first=25, keep_last=25):\n",
        "    words = tweet.split()  # Split tweet into words\n",
        "    if len(words) > max_words:\n",
        "        # Keep the first `keep_first` words and the last `keep_last` words\n",
        "        truncated = \" \".join(words[:keep_first] + words[-keep_last:])\n",
        "    else:\n",
        "        # If the tweet is already short, return it as-is\n",
        "        truncated = tweet\n",
        "    return truncated\n",
        "\n",
        "tweets = [\n",
        "    \"This is a short tweet.\",\n",
        "    \"This tweet is quite long, and it goes on and on about various things. It exceeds the fifty-word limit, and we want to make sure it is truncated properly by keeping the first twenty-five words and the last twenty-five words. This is a test to see if the function does that. Let's hope that it does just that. Let's make the tweet even longer, since fifty words is actually a lot. So, now I think the tweet is long enough\",\n",
        "    \"Another reasonably short tweet for testing purposes.\",\n",
        "]\n",
        "\n",
        "# Apply truncation to all tweets\n",
        "processed_tweets = [truncate_long_tweets(tweet) for tweet in tweets]\n",
        "\n",
        "# Print results\n",
        "for i, tweet in enumerate(processed_tweets, 1):\n",
        "    print(f\"Tweet {i}: {tweet}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33hBAn7nbP2_"
      },
      "source": [
        "# Rebuilding the Vocabulary af Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDqCoJz-bR8k",
        "outputId": "495ec104-6712-42ca-b7f6-5adf1ee5a998"
      },
      "outputs": [],
      "source": [
        "tokenized_texts = [text.split() for text in train_texts]\n",
        "unique_tokens = set(word for tweet in tokenized_texts for word in tweet)\n",
        "print(len(unique_tokens), \"unique tokens in training set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cblFalolbcpt",
        "outputId": "e88828a3-e30d-420f-8e19-fc8f8042ac10"
      },
      "outputs": [],
      "source": [
        "train_texts = hf_train['text']\n",
        "\n",
        "# 1. Tokenize everything in your training set\n",
        "tokenized_texts = [text.split() for text in train_texts]\n",
        "\n",
        "# 2. Flatten the list of lists to get a single list of all tokens\n",
        "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
        "\n",
        "# 3. Count frequencies\n",
        "freq_counter = Counter(all_tokens)\n",
        "print(f\"Number of unique tokens: {len(freq_counter)}\")\n",
        "\n",
        "# 4. Sort words by descending frequency\n",
        "sorted_token_freqs = freq_counter.most_common()  # list of (word, count), sorted descending\n",
        "total_tokens = sum(freq_counter.values())        # total occurrences of all tokens\n",
        "\n",
        "print(f\"Total token occurrences in train set: {total_tokens}\")\n",
        "\n",
        "# 5. Compute cumulative coverage\n",
        "cumulative = 0\n",
        "coverage_break = 0.95\n",
        "needed_words = 0\n",
        "\n",
        "for i, (token, count) in enumerate(sorted_token_freqs, start=1):\n",
        "    cumulative += count\n",
        "    if (cumulative / total_tokens) >= coverage_break:\n",
        "        needed_words = i\n",
        "        break\n",
        "\n",
        "print(f\"To cover {coverage_break*100}% of word occurrences, you need ~{needed_words} words.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtB2iH0RbhC9",
        "outputId": "9da01f8b-6bae-4fd3-9282-60a95889255a"
      },
      "outputs": [],
      "source": [
        "train_texts = hf_train['text']\n",
        "val_texts   = hf_validation['text']\n",
        "test_texts  = hf_test['text']\n",
        "\n",
        "y_train = hf_train['label']\n",
        "y_val   = hf_validation['label']\n",
        "y_test  = hf_test['label']\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(val_texts))\n",
        "print(len(test_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1z4_-nKbk3x",
        "outputId": "8564198b-779d-4c2d-a516-2718a8be2db7"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 6000\n",
        "\n",
        "encoder = TextVectorization(\n",
        "    max_tokens=MAX_VOCAB_SIZE,\n",
        "    output_mode='int'\n",
        ")\n",
        "\n",
        "encoder.adapt(train_texts)  # Build vocabulary from training texts\n",
        "\n",
        "\n",
        "# 2. Inspect the Vocabulary\n",
        "\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "print(f\"First 50 words in the vocabulary:\\n{vocab[:50]}\\n\")\n",
        "print(f\"Vocabulary length: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9sQzA2ubn6i",
        "outputId": "f2bfcad8-d024-499e-f896-fe4cae2a0b4b"
      },
      "outputs": [],
      "source": [
        "# Encode each split separately\n",
        "encoded_train = encoder(train_texts).numpy()\n",
        "encoded_val   = encoder(val_texts).numpy()\n",
        "encoded_test  = encoder(test_texts).numpy()\n",
        "\n",
        "print(\"Example of encoded train data:\", encoded_train[0])\n",
        "print(\"Shape of encoded train:\", encoded_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbakRqIXco8j",
        "outputId": "7a8b8a46-767e-4cad-c620-213bcc2b86ae"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 50\n",
        "\n",
        "X_train = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating='post')\n",
        "X_val   = pad_sequences(encoded_val, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test  = pad_sequences(encoded_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "print(hf_train['text'][0])\n",
        "print(\"Example of encoded train data:\", X_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQWoFVGcdBBc"
      },
      "source": [
        "# Training the Models again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8eSfm1JdilG",
        "outputId": "526bdf3d-c60d-4e66-d79a-c0743639c460"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype('int32')\n",
        "X_val   = X_val.astype('int32')\n",
        "X_test  = X_test.astype('int32')\n",
        "\n",
        "print(X_train.dtype)\n",
        "print(X_train.shape)\n",
        "\n",
        "y_train = np.array(y_train, dtype='int32')\n",
        "y_val = np.array(y_val, dtype='int32')\n",
        "y_test = np.array(y_test, dtype='int32')\n",
        "\n",
        "print(y_train.dtype)\n",
        "print(y_train.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG8GDzlkdXXY",
        "outputId": "437716de-c194-46c8-b6bd-56daf29bc55d"
      },
      "outputs": [],
      "source": [
        "# Bidirectional GRU\n",
        "history = Bidirectional_GRU_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "MVKzld0kgSX0",
        "outputId": "9b42bc87-ac12-478b-f7f9-9cb4af14cbfd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56DgBRoPefC4",
        "outputId": "2e02d730-5579-4fa0-a629-49703af11f44"
      },
      "outputs": [],
      "source": [
        "# Bidirectional LSTM\n",
        "history = Bidirectional_LSTM_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    class_weight=weights,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "RlQW5-JegUQo",
        "outputId": "d436059b-1a41-4cab-ddbf-cb8474d92f90"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et9RF-S7f2F6",
        "outputId": "d3e35455-1012-4a93-b175-0c32f666610b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Bidirectional GRU\n",
        "bidir_gru_predictions = Bidirectional_GRU_model.predict(X_test)\n",
        "bidir_gru_predicted_labels = bidir_gru_predictions.argmax(axis=1)\n",
        "bidirectional_gru_accuracy = accuracy_score(y_test, bidir_gru_predicted_labels)\n",
        "\n",
        "# Bidirectional LSTM\n",
        "bidir_lstm_predictions = Bidirectional_LSTM_model.predict(X_test)\n",
        "bidir_lstm_predicted_labels = bidir_lstm_predictions.argmax(axis=1)\n",
        "bidirectional_lstm_accuracy = accuracy_score(y_test, bidir_lstm_predicted_labels)\n",
        "\n",
        "\n",
        "print(f\"Bidirectional GRU Test Accuracy: {bidirectional_gru_accuracy:.4f}\")\n",
        "print(f\"Bidirectional LSTM Test Accuracy: {bidirectional_lstm_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tc10QFBgETi",
        "outputId": "193b145f-fd93-4d00-a841-90d05ee14e58"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Bidirectional GRU\n",
        "test_predicted_labels_bidir_gru = bidir_gru_predictions.argmax(axis=1)\n",
        "test_classification_report_bidir_gru = classification_report(y_test, test_predicted_labels_bidir_gru, target_names=label_mapping.values())\n",
        "\n",
        "# Bidirectional LSTM\n",
        "test_predicted_labels_bidir_lstm = bidir_lstm_predictions.argmax(axis=1)\n",
        "test_classification_report_bidir_lstm = classification_report(y_test, test_predicted_labels_bidir_lstm, target_names=label_mapping.values())\n",
        "\n",
        "print(\"Classification Report for Bidirectional GRU:\")\n",
        "print(test_classification_report_bidir_gru)\n",
        "\n",
        "print(\"Classification Report for Bidirectional LSTM:\")\n",
        "print(test_classification_report_bidir_lstm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "e_FSZG0UpMJF",
        "outputId": "96ba5d04-6760-487c-b82b-d2ca0e7d422e"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "heatmap_colors = {\n",
        "    \"Standard RNN\": \"Blues\",\n",
        "    \"Bidirectional RNN\": \"Blues\",\n",
        "    \"Standard GRU\": \"Greens\",\n",
        "    \"Bidirectional GRU\": \"Greens\",\n",
        "    \"Standard LSTM\": \"Reds\",\n",
        "    \"Bidirectional LSTM\": \"Reds\"\n",
        "}\n",
        "\n",
        "# Bidirectional GRU\n",
        "cm_bidir_gru = confusion_matrix(y_test, test_predicted_labels_bidir_gru)\n",
        "sns.heatmap(cm_bidir_gru, annot=True, fmt='d', cmap=heatmap_colors[\"Bidirectional GRU\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Bidirectional GRU')\n",
        "plt.show()\n",
        "\n",
        "# Bidirectional LSTM\n",
        "cm_bidir_lstm = confusion_matrix(y_test, test_predicted_labels_bidir_lstm)\n",
        "sns.heatmap(cm_bidir_lstm, annot=True, fmt='d', cmap=heatmap_colors[\"Bidirectional LSTM\"], xticklabels=label_mapping.values(), yticklabels=label_mapping.values())\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Bidirectional LSTM')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_qv5NIYpSz6"
      },
      "source": [
        "# Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "55cxoj3Bpi9g",
        "outputId": "12ab5d5d-5446-4634-dcdd-b7c3e444a1ba"
      },
      "outputs": [],
      "source": [
        "# Average the probabilities from the Bidirectional GRU and Bidirectional LSTM\n",
        "ensemble_probs = (bidir_gru_predictions + bidir_lstm_predictions) / 2\n",
        "\n",
        "# Get the final predicted labels from the ensemble\n",
        "ensemble_predicted_labels = ensemble_probs.argmax(axis=1)\n",
        "\n",
        "# Calculate and display ensemble accuracy\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predicted_labels)\n",
        "print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "# Classification report for the ensemble\n",
        "test_classification_report_ensemble = classification_report(\n",
        "    y_test, ensemble_predicted_labels, target_names=label_mapping.values()\n",
        ")\n",
        "print(\"Classification Report for Ensemble:\")\n",
        "print(test_classification_report_ensemble)\n",
        "\n",
        "# Confusion matrix for the ensemble\n",
        "cm_ensemble = confusion_matrix(y_test, ensemble_predicted_labels)\n",
        "sns.heatmap(\n",
        "    cm_ensemble, annot=True, fmt='d', cmap='Purples',  # Use \"Purples\" for purple color\n",
        "    xticklabels=label_mapping.values(), yticklabels=label_mapping.values()\n",
        ")\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Ensemble')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ab0682e2cbf44549a2c42944e6aec2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e58a4178f1d42a2992e60dfe8e2663d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1533e227b544769ad2c220758febd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a795317f1f44175add0fbb8911888ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ae73fdfe6514a039011ee5e64661b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_965e03b85d82450aa9819b77dcee33ec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ae920dd0bd984ea6a988381244e97c22",
            "value": "‚Äá299527/299527‚Äá[00:40&lt;00:00,‚Äá7630.26‚Äáexamples/s]"
          }
        },
        "3c598d22102548eeb5d2f5e91c9e69a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51d874efcf5a4f02a8070a90cac8fce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d1533e227b544769ad2c220758febd8",
            "max": 299527,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c598d22102548eeb5d2f5e91c9e69a8",
            "value": 299527
          }
        },
        "5bba9faa56e94e8498737560513aecc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0d4be4bf684c74af2b9039c68d75bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "652a21682c6a492d802a5e35a5263921": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6097e4d5b7740f8812177f4a8519acd",
              "IPY_MODEL_51d874efcf5a4f02a8070a90cac8fce8",
              "IPY_MODEL_2ae73fdfe6514a039011ee5e64661b0f"
            ],
            "layout": "IPY_MODEL_0ab0682e2cbf44549a2c42944e6aec2e"
          }
        },
        "7e1f8343b2d649b59881c164c004f1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e76f0daf94da49c9b6faa0ddd489053c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7f744c3cafdb4fa9b9af51875bcd27f6",
            "value": "Map:‚Äá100%"
          }
        },
        "7f744c3cafdb4fa9b9af51875bcd27f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "857969b4464a4772b8d8bb6b5471fe55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965e03b85d82450aa9819b77dcee33ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9fce9e2a43949b2a2ecc23d2302d911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae920dd0bd984ea6a988381244e97c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b7ae00415445e0aba57b3fdf2fddb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e1f8343b2d649b59881c164c004f1ec",
              "IPY_MODEL_d91d200d7b6b44c89723da7e0f02c83b",
              "IPY_MODEL_cefb8ff0139243de98eb6615a67bdca4"
            ],
            "layout": "IPY_MODEL_0e58a4178f1d42a2992e60dfe8e2663d"
          }
        },
        "cefb8ff0139243de98eb6615a67bdca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bba9faa56e94e8498737560513aecc3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5f0d4be4bf684c74af2b9039c68d75bf",
            "value": "‚Äá299527/299527‚Äá[00:03&lt;00:00,‚Äá87737.50‚Äáexamples/s]"
          }
        },
        "d6097e4d5b7740f8812177f4a8519acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6215d21e224998be36401e69436ae1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2a795317f1f44175add0fbb8911888ff",
            "value": "Map:‚Äá100%"
          }
        },
        "d91d200d7b6b44c89723da7e0f02c83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_857969b4464a4772b8d8bb6b5471fe55",
            "max": 299527,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9fce9e2a43949b2a2ecc23d2302d911",
            "value": 299527
          }
        },
        "e76f0daf94da49c9b6faa0ddd489053c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6215d21e224998be36401e69436ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
